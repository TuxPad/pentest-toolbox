import argparse
import sys
from urllib.parse import urlparse, urlunparse
from tabulate import tabulate
from tqdm import tqdm
import requests

# Arguments
parser = argparse.ArgumentParser()
parser.add_argument("-f", "--file", required=True, help="File of payloads to be used")
parser.add_argument("-fo", "--filter", action="store_true", help="Removes 404")
#parser.add_argument("-H", "--header", help="Add custom header")
parser.add_argument("-o", "--output", help="Save output to file")
parser.add_argument("-r", "--recursive", action="store_true", help="Recursive subdomain fuzzing")
parser.add_argument("-u", "--URL",required=True, help="https://url.to.site/to/be/fuzzed")
parser.add_argument("-v", "--version", action="version", version="tfuzz version: 0.1",
                    help="Show program's version number and exit.")

args = parser.parse_args()
target = urlparse(args.URL)

# Load the file containing payloads
def read_payloads(inp):
    p_list = []
    try:
        with open(inp, 'r', encoding='UTF-8') as file:
            for line in file:
                p_list.append(line.strip())
    except FileNotFoundError:
        sys.exit("Payload file not found")
    return p_list

# Try to connect to the specified URL and return response
def conn(payload,url):
    try:
        response = requests.get(url.replace('FUZZ', payload))
    except:
        pass
    if args.filter and response.status_code == 404:
        return None
    return response



def fuzz(p_load, url):
    # Setup progress bar
    if args.recursive:
        if urlparse(url).netloc.split('.')[0] != 'FUZZ':
            print(urlparse(url).netloc)
            sys.exit("Recursion only works on subdomain fuzzing")
        pbar = tqdm(total=len(p_load)*(len(p_load)))
    else:
        pbar = tqdm(total=len(p_load))

    result = []
    # Loop through the payloads and try to connect.
    for i in p_load:
        pbar.set_description(i)
        try:
            response = conn(i,url)
            result.append((i,response.status_code,response.elapsed.total_seconds()*1000))
        except:
            pass
        pbar.update(1)
        # If (faux) recursion on subdomains, try one level deeper.
        if args.recursive:
            new_url = 'https://FUZZ.' + i + urlparse(url).netloc.strip('FUZZ')
            new_p_load = p_load[:]
            for j in new_p_load:
                pbar.set_description(j+'.'+i)
                try:
                    response_r = conn(j,new_url)
                    result.append((j+'.'+i,
                                   response_r.status_code,response_r.elapsed.total_seconds()*1000))
                except:
                    pass
                pbar.update(1)



    pbar.close()
    return result


def main():
    payloads = read_payloads(args.file)
    output = fuzz(payloads, urlunparse(target))
    print(f"Results for {args.URL}:")
    print(tabulate(output,headers=["Payload:","Status code:","Response time:"]))

    # Saves output to file.
    if args.output:
        with open(args.output, 'w', encoding="UTF-8") as output_file:
            output_file.write('\n'.join(str(i) for i in output))

if __name__ == "__main__":
    main()
